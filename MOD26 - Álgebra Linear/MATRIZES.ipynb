{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9a1ddc-5119-48c8-b786-9b37fba21212",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f32011-a4bc-4b6c-995d-f6554b26e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato da Matriz (Shape): (3, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criando um vetor e uma matriz\n",
    "vetor = np.array([1, 2, 3])\n",
    "matriz = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Operações comuns\n",
    "transposta = matriz.T # Inverte linhas por colunas\n",
    "produto = np.dot(matriz, np.array([10, 20])) # Produto escalar/matricial\n",
    "\n",
    "print(f\"Formato da Matriz (Shape): {matriz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aec362b-c11d-4101-aab8-d122bf60adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições (Vetor y_hat): [21.  18.  32.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Matriz de Design X (Ex: 3 amostras, 2 variáveis explicativas)\n",
    "# Adicionamos uma coluna de 1s no início para representar o intercepto (beta0)\n",
    "X = np.array([\n",
    "    [1, 0.5, 2.0],\n",
    "    [1, 1.5, 1.0],\n",
    "    [1, 2.5, 3.5]\n",
    "])\n",
    "\n",
    "# 2. Vetor de Coeficientes beta (aprendidos pelo modelo)\n",
    "# beta0 = 10, beta1 = 2, beta2 = 5\n",
    "beta = np.array([10, 2, 5])\n",
    "\n",
    "# 3. Operação de Multiplicação (Predição)\n",
    "# Usamos o operador @ ou np.dot\n",
    "y_pred = X @ beta\n",
    "\n",
    "print(\"Predições (Vetor y_hat):\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b95bfb-ec6d-425e-9530-c33b6bc429ba",
   "metadata": {},
   "source": [
    "### Por que multiplicar Matriz por Vetor na Regressão?\n",
    "\n",
    "1. **Eficiência:** Em vez de calcular a previsão linha por linha em um loop, a Álgebra Linear permite que o computador processe milhares de linhas simultaneamente usando operações de baixo nível da CPU/GPU.\n",
    "2. **Notação Compacta:** A equação $y = X\\beta$ substitui fórmulas gigantescas, facilitando a derivação de modelos complexos.\n",
    "3. **Representação:** Cada linha da matriz $X$ representa um objeto (ex: um cliente), e o vetor $\\beta$ representa a importância de cada atributo para a previsão final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368212f7-3fba-4515-a235-0ca33d96f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1 1]\n",
      "Escalonado: [2.  0.5]\n",
      "Rotacionado: [-1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Nosso dado original (um ponto no espaço 2D)\n",
    "v = np.array([1, 1])\n",
    "\n",
    "# 1. Matriz de Escalonamento (Aumenta X em 2x e reduz Y em 0.5x)\n",
    "S = np.array([\n",
    "    [2, 0],\n",
    "    [0, 0.5]\n",
    "])\n",
    "\n",
    "# 2. Matriz de Rotação (Gira 90 graus no sentido anti-horário)\n",
    "R = np.array([\n",
    "    [0, -1],\n",
    "    [1,  0]\n",
    "])\n",
    "\n",
    "# Aplicando as transformações\n",
    "v_escalonado = S @ v\n",
    "v_rotacionado = R @ v\n",
    "\n",
    "print(f\"Original: {v}\")\n",
    "print(f\"Escalonado: {v_escalonado}\")\n",
    "print(f\"Rotacionado: {v_rotacionado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090d5ae-300f-49cf-9b6a-68b09cf54568",
   "metadata": {},
   "source": [
    "### Álgebra Linear e Transformações de Dados\n",
    "\n",
    "Nesta etapa, entendemos que:\n",
    "1. **Multiplicação de Matrizes** permite combinar operações. Se eu quero rotacionar e depois escalonar um dado, posso multiplicar as duas matrizes de transformação e aplicar o resultado uma única vez.\n",
    "2. **Transformação Linear** é a base matemática para o pré-processamento de dados. Quando normalizamos uma variável, estamos aplicando uma transformação linear de escalonamento.\n",
    "3. **Perspectiva Geométrica:** No Machine Learning, transformar a matriz de dados é muitas vezes o segredo para tornar um problema complexo em algo linearmente separável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecffca1-2db4-4e75-927e-9fff9e1db2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
